# ATIVIDADE (IR AL√âM) ‚Äì Da Terra ao C√≥digo: Automatizando a Classifica√ß√£o de Gr√£os com Machine Learning
---

Integrantes (Nome + RM)
Leticia Grossi Dornelas ‚Äì RM568172
Leonardo Borges Alves da Mota ‚Äì RM566939
Bernardo Naves Doti Avelar ‚Äì RM566867
David Eduardo da Silva Correia - RM567525

---


# Classifica√ß√£o de Gr√£os de Trigo ‚Äì Machine Learning

Este projeto tem como objetivo analisar um conjunto de dados contendo medi√ß√µes f√≠sicas de gr√£os de trigo e desenvolver modelos de **classifica√ß√£o supervisionada** capazes de distinguir tr√™s diferentes variedades:

- **Kama**
- **Rosa**
- **Canadian**

O trabalho segue as etapas cl√°ssicas de um fluxo de Machine Learning:  
**An√°lise Explorat√≥ria (EDA) ‚Üí Pr√©-processamento ‚Üí Modelagem ‚Üí Avalia√ß√£o ‚Üí Otimiza√ß√£o ‚Üí Interpreta√ß√£o final.**

---

## üìÅ Estrutura do Reposit√≥rio

/
‚îú‚îÄ‚îÄ seeds_dataset.txt # Conjunto de dados utilizado
‚îú‚îÄ‚îÄ fase_04_ctwp_cap3.ipynb # Notebook com an√°lise e modelos
‚îú‚îÄ‚îÄ fase_04_ctwp_cap3.py # Vers√£o exportada do notebook
‚îî‚îÄ‚îÄ README.md # Documenta√ß√£o do projeto


---

## 1. Descri√ß√£o do Conjunto de Dados

O dataset cont√©m **210 amostras**, cada uma representando um gr√£o de trigo descrito por **7 caracter√≠sticas f√≠sicas**:

1. **area** ‚Äì √°rea do gr√£o  
2. **perimeter** ‚Äì per√≠metro do gr√£o  
3. **compactness** ‚Äì medida de compacidade  
4. **length_kernel** ‚Äì comprimento do n√∫cleo  
5. **width_kernel** ‚Äì largura do n√∫cleo  
6. **asymmetry_coefficient** ‚Äì coeficiente de assimetria  
7. **length_groove** ‚Äì comprimento do sulco do n√∫cleo  

A vari√°vel alvo (**class**) possui 3 r√≥tulos:

- 1 = Kama  
- 2 = Rosa  
- 3 = Canadian  

---

## 2. An√°lise Explorat√≥ria e Pr√©-processamento

As seguintes etapas foram executadas:

### ‚úî Inspe√ß√£o inicial
- Verifica√ß√£o de dimens√µes, tipos de dados e contagem de classes.
- Dataset completamente **balanceado**: 70 amostras por classe.

### ‚úî Estat√≠sticas descritivas
- C√°lculo de m√©dia, mediana e desvio padr√£o.
- Atributos mostram varia√ß√£o coerente entre as variedades.

### ‚úî Visualiza√ß√µes
- **Histogramas** e **boxplots** para identificar distribui√ß√µes e outliers.
- **Pairplot** para observar separabilidade entre classes.
- **Matriz de correla√ß√£o** mostrando forte rela√ß√£o entre atributos de tamanho.

### ‚úî Tratamento de dados
- N√£o foram encontrados valores ausentes.
- Padroniza√ß√£o aplicada com **StandardScaler**.

---

## üß† 3. Modelos de Classifica√ß√£o Treinados

Foram utilizados cinco algoritmos:

- **K-Nearest Neighbors (KNN)**
- **Support Vector Machine (SVM ‚Äì RBF)**
- **Random Forest**
- **Logistic Regression**
- **Naive Bayes**

### ‚úî Divis√£o dos dados
- 70% para treino  
- 30% para teste  
- Estratifica√ß√£o mantida  

### ‚úî M√©tricas avaliadas
- **Acur√°cia**
- **Precis√£o (macro)**
- **Recall (macro)**
- **F1-score (macro)**
- **Matriz de confus√£o**

---

## 4. Resultados dos Modelos (Sem Otimiza√ß√£o)

Com base nos resultados obtidos:

- **Random Forest** apresentou o melhor desempenho geral (~0.92).
- **KNN** e **SVM (RBF)** obtiveram m√©tricas semelhantes (~0.87).
- **Logistic Regression** veio logo abaixo (~0.85).
- **Naive Bayes** teve o menor desempenho (~0.80), mas ainda razo√°vel.

As matrizes de confus√£o indicaram que erros tendem a ocorrer entre classes morfologicamente semelhantes.

---

## ‚öô 5. Otimiza√ß√£o via Grid Search

Foram ajustados hiperpar√¢metros de:

- **KNN** ‚Üí n_neighbors, weights, metric  
- **SVM (RBF)** ‚Üí C, gamma  
- **Random Forest** ‚Üí n_estimators, max_depth, min_samples_split  

### ‚úî Resultados da otimiza√ß√£o
- Melhoras leves a moderadas nas m√©tricas.
- **Random Forest otimizado** manteve o melhor desempenho final.
- **SVM** tamb√©m apresentou ganhos com ajuste fino de C e gamma.

---

## 6. Conclus√µes

- O dataset de gr√£os possui **alta separabilidade**, facilitando a classifica√ß√£o.
- **Random Forest** destacou-se como o melhor modelo:
  - Alta acur√°cia
  - Robustez a outliers
  - Boa capacidade de generaliza√ß√£o  
- A padroniza√ß√£o foi essencial para modelos baseados em dist√¢ncia (KNN e SVM).
- Os resultados confirmam que m√©todos supervisionados s√£o eficazes para classificar variedades de trigo com base em morfologia.

---

## 7. Como Executar o Projeto

### **No Google Colab**
1. Abra o notebook `.ipynb`
2. Fa√ßa upload do arquivo `seeds_dataset.txt`
3. Execute todas as c√©lulas na ordem correta

### **No computador**
Instale as depend√™ncias:

```bash
pip install numpy pandas matplotlib seaborn scikit-learn
